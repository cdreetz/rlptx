Logging to ./qwen_triton_sft/logs
Logging into HF

    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

[1m[31mCannot authenticate through git-credential as no helper is defined on your machine.
You might have to re-authenticate when pushing to the Hugging Face Hub.
Run the following command in your terminal in case you want to set the 'store' credential helper as default.

git config --global credential.helper store

Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.[0m
Token has not been saved to git credential helper.
Loading dataset from cdreetz/triton-sft-dataset...
Loaded 989 examples
Train: 890, Eval: 99
Epoch 1:   0%|â–Ž                                                                                                                                                          | 1/445 [00:03<23:41,  3.20s/it]/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Evaluating:   0%|                                                                                                                                                                 | 0/50 [00:00<?, ?it/s]
Epoch 1:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                      | 99/445 [00:23<01:23,  4.15it/s, loss=0.1929, ppl=1.21, lr=1.52e-05, grad_norm=1.320, eff_bs=4]
Traceback (most recent call last):
  File "/home/ubuntu/rlptx/sft_grpo/trainer2.py", line 458, in <module>
    model = trainer_setup.train(
  File "/home/ubuntu/rlptx/sft_grpo/trainer2.py", line 349, in train
    train_loss = self.train_epoch(
  File "/home/ubuntu/rlptx/sft_grpo/trainer2.py", line 241, in train_epoch
    eval_loss, eval_perplexity = self.evaluate(model, eval_dataloader)
  File "/home/ubuntu/rlptx/sft_grpo/trainer2.py", line 269, in evaluate
    with autocast():
TypeError: autocast.__init__() missing 1 required positional argument: 'device_type'
