[
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise subtract of two 1D tensors of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "subtract",
    "input_shapes": [
      [
        16384
      ],
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise subtract of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "subtract",
    "input_shapes": [
      [
        8192
      ],
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the min reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    stride_M, \n    stride_N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    output_row = row\n    input_row = row * stride_M\n    \n    col = tl.arange(0, BLOCK_SIZE_N)\n    input_col = col\n    \n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    mask = col < N\n    \n    for _ in range(0, N, BLOCK_SIZE_N):\n        input_ptr_masked = input_ptr + input_row + input_col\n        val = tl.load(input_ptr_masked, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        input_col += BLOCK_SIZE_N\n        mask = (col + BLOCK_SIZE_N) < N\n    \n    min_val = tl.min(min_val, axis=0)\n    output_ptr_masked = output_ptr + output_row\n    tl.store(output_ptr_masked, min_val)\n\ndef min_reduce(input, output):\n    M, N = input.shape\n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 256\n    min_reduce_kernel[grid](input, output, M, N, input.stride(0), input.stride(1), BLOCK_SIZE_N=BLOCK_SIZE_N)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    stride_M, \n    stride_N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    output_row = row\n    input_row = row * stride_M\n    \n    col = tl.arange(0, BLOCK_SIZE_N)\n    input_col = col\n    \n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    mask = col < N\n    \n    for _ in range(0, N, BLOCK_SIZE_N):\n        input_ptr_masked = input_ptr + input_row + input_col\n        val = tl.load(input_ptr_masked, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        input_col += BLOCK_SIZE_N\n        mask = (col + BLOCK_SIZE_N) < N\n    \n    min_val = tl.min(min_val, axis=0)\n    output_ptr_masked = output_ptr + output_row\n    tl.store(output_ptr_masked, min_val)\n\ndef min_reduce(input, output):\n    M, N = input.shape\n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 256\n    min_reduce_kernel[grid](input, output, M, N, input.stride(0), input.stride(1), BLOCK_SIZE_N=BLOCK_SIZE_N)",
    "operation": "min_reduction",
    "input_shapes": [
      [
        512,
        256
      ]
    ],
    "output_shape": [
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    input_ptr = input_ptr + row * N\n    output_ptr = output_ptr + row\n\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    input_ptrs = input_ptr + col_offsets + col_block * BLOCK_SIZE_N\n\n    accumulator = tl.zeros((1,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offsets + col_block * BLOCK_SIZE_N < N\n        values = tl.load(input_ptrs, mask=mask, other=0.0)\n        accumulator += tl.sum(values, axis=0)\n        col_block += 1\n        input_ptrs += BLOCK_SIZE_N\n\n    tl.store(output_ptr, accumulator)\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    output = torch.empty(M, device=x.device, dtype=torch.float32)\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    input_ptr = input_ptr + row * N\n    output_ptr = output_ptr + row\n\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    input_ptrs = input_ptr + col_offsets + col_block * BLOCK_SIZE_N\n\n    accumulator = tl.zeros((1,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offsets + col_block * BLOCK_SIZE_N < N\n        values = tl.load(input_ptrs, mask=mask, other=0.0)\n        accumulator += tl.sum(values, axis=0)\n        col_block += 1\n        input_ptrs += BLOCK_SIZE_N\n\n    tl.store(output_ptr, accumulator)\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    output = torch.empty(M, device=x.device, dtype=torch.float32)\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        1024,
        256
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_PTR, B_PTR, C_PTR,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_PTR + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_PTR + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_PTR + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_PTR, B_PTR, C_PTR,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_PTR + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_PTR + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_PTR + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        256
      ],
      [
        256,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and number of programs\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers to the current block in A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block in C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load current block from A and B with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform dot product for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Create pointer to the current block in C\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store the result with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the input shapes are compatible\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n\n    # Get the dimensions of the input matrices\n    M, K = a.shape\n    N = b.shape[1]\n\n    # Allocate output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define grid for the kernel launch\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and number of programs\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers to the current block in A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block in C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load current block from A and B with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform dot product for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Create pointer to the current block in C\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store the result with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the input shapes are compatible\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n\n    # Get the dimensions of the input matrices\n    M, K = a.shape\n    N = b.shape[1]\n\n    # Allocate output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define grid for the kernel launch\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        256
      ],
      [
        256,
        512
      ]
    ],
    "output_shape": [
      512,
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  }
]