[
  {
    "prompt": "Can you point me to a Triton kernel that performs a simple element-wise addition of two tensors? I'm trying to optimize some numerical computations and I'd love to see how it's done in Triton.",
    "operation": "add",
    "tier": 1,
    "reference_op": "torch.add",
    "input_shapes": [[256]],
    "output_shape": [256]
  },
  {
    "prompt": "Hey, I'm working on a project that involves some custom GPU computations and I'm using Triton. Can you point me to a simple example of a kernel that just adds two tensors together? I'm having trouble figuring out the syntax for it.",
    "operation": "add",
    "tier": 1,
    "reference_op": "torch.add",
    "input_shapes": [[512]],
    "output_shape": [512]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some matrix operations and I need a Triton kernel that can handle element-wise multiplication. Can someone point me to a simple example or provide a basic implementation that just multiplies two tensors?\"",
    "operation": "multiply",
    "tier": 1,
    "reference_op": "torch.mul",
    "input_shapes": [[128]],
    "output_shape": [128]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some matrix operations and I was wondering if there's a Triton kernel that can handle a simple element-wise multiplication for me?\"",
    "operation": "multiply",
    "tier": 1,
    "reference_op": "torch.mul",
    "input_shapes": [[4096]],
    "output_shape": [4096]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some numerical computations and I need a kernel that can compute the exponential function efficiently on the GPU. Does Triton have a built-in kernel for exp or would I need to write my own? If not, can someone point me to a simple example of how to implement it?\"",
    "operation": "exp",
    "tier": 1,
    "reference_op": "torch.exp",
    "input_shapes": [[1536]],
    "output_shape": [1536]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of exponential calculations and I was wondering if we have a Triton kernel that can handle that efficiently? Maybe something that's already optimized for our hardware?\"",
    "operation": "exp",
    "tier": 1,
    "reference_op": "torch.exp",
    "input_shapes": [[512]],
    "output_shape": [512]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of summation operations and I was wondering if there's a Triton kernel that can handle summing up elements in a tensor efficiently? Specifically, I'm looking for something that can sum along a particular axis. Does anyone have an example or a pointer to a resource that could help me implement this?\"",
    "operation": "sum",
    "tier": 1,
    "reference_op": "torch.sum",
    "input_shapes": [[128]],
    "output_shape": []
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of element-wise summations and I was wondering if there's a Triton kernel that can handle that efficiently? Something that can take in two large tensors and just add them together, you know, without having to write a bunch of custom CUDA code.\"",
    "operation": "sum",
    "tier": 1,
    "reference_op": "torch.sum",
    "input_shapes": [[1536]],
    "output_shape": []
  },
  {
    "prompt": "Can you hook me up with a Triton kernel that just does a simple max reduction? I'm trying to optimize some code and I need something that's super fast.\n\nI'm looking for a Triton kernel that can efficiently compute the maximum value across a large tensor - do you have any examples or resources I could use?\n\nHow hard would it be to write a Triton kernel that performs a max operation? I'm pretty new to Triton and could use some guidance.\n\nDo you have a Triton kernel template for reductions, specifically max? That would be a huge help.\n\nI need a Triton kernel for a max pooling operation - can anyone provide an example or point me in the right direction?\n\nIs there a straightforward way to implement a max kernel in Triton? I'm having some trouble figuring out the best approach.",
    "operation": "max",
    "tier": 1,
    "reference_op": "torch.max",
    "input_shapes": [[256]],
    "output_shape": []
  },
  {
    "prompt": "\"Hey, I'm trying to optimize this reduction operation and I need a kernel that just computes the maximum value across a tensor. Can I get a Triton kernel that does that? Something simple and efficient would be great.\"\n\n\"Can you provide a Triton kernel implementation for a max reduction operation? I'm looking for something that's highly performant and can handle large input tensors.\"\n\n\"I'm trying to replace this PyTorch reduction with a custom Triton kernel. The operation is just a simple max across a dimension. Do you have an example Triton kernel that I could use as a starting point?\"\n\n\"What's the best way to write a Triton kernel for computing the maximum value in a tensor? Are there any existing examples or templates I could leverage?\"\n\n\"I need a Triton kernel that performs a max reduction. Can someone point me to a reference implementation or provide a simple example that I could build upon?\"",
    "operation": "max",
    "tier": 1,
    "reference_op": "torch.max",
    "input_shapes": [[768]],
    "output_shape": []
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some matrix operations and I was wondering if there's a Triton kernel that can handle transposes efficiently? Something that can handle large matrices would be great. Does that exist?\"\n\n\"Can someone point me to a Triton kernel implementation that performs a matrix transpose? I'm looking for something that's highly optimized and can be used as a building block for other linear algebra operations.\"\n\n\"I'm working on a project that involves a lot of matrix transposes and I'm considering using Triton. Does anyone have an example of a Triton kernel that does a transpose? Ideally, it would be something that's easy to integrate into my existing codebase.\"\n\n\"Is there a standard Triton kernel for transposing matrices that I can use as a reference? I'm trying to implement some custom linear algebra operations and I want to make sure I'm not reinventing the wheel.\"\n\n\"I'm looking for a high-performance transpose kernel in Triton. Can anyone suggest a good implementation or point me in the direction of some relevant resources?\"",
    "operation": "transpose",
    "tier": 1,
    "reference_op": "torch.transpose",
    "input_shapes": [[256, 768]],
    "output_shape": [768, 256]
  },
  {
    "prompt": "\"Hey, I'm working on optimizing some matrix operations and I was wondering if there's a Triton kernel that can handle transposes efficiently - something that can handle large matrices and is pretty flexible with data types?\"",
    "operation": "transpose",
    "tier": 1,
    "reference_op": "torch.transpose",
    "input_shapes": [[1024, 768]],
    "output_shape": [768, 1024]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of matrix-vector multiplications and I was wondering if there's a Triton kernel available that can handle gemv operations efficiently? If not, what would be the best way to implement one?\"",
    "operation": "gemv",
    "tier": 1,
    "reference_op": "torch.mv",
    "input_shapes": [[128, 1024], [1024]],
    "output_shape": [128]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of matrix-vector multiplications and I'm thinking of using Triton for the kernel. Can someone point me to a gemv kernel implementation or give me a rough idea of how to write one? I've been digging through the Triton docs but it's a bit overwhelming.\"",
    "operation": "gemv",
    "tier": 1,
    "reference_op": "torch.mv",
    "input_shapes": [[256, 1024], [1024]],
    "output_shape": [256]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some matrix multiplication code and I heard Triton can really help with that. Can someone point me to a kernel that does a basic gemm operation? Maybe something that's already been tuned for performance?\"",
    "operation": "gemm",
    "tier": 1,
    "reference_op": "torch.matmul",
    "input_shapes": [
      [768, 256],
      [256, 256]
    ],
    "output_shape": [768, 256]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize some matrix multiplication code and I was wondering if there's a Triton kernel out there that already does gemm efficiently?\"\n\n\"Can someone point me to a Triton implementation of a general matrix multiply kernel? I'm looking for something that's well-optimized and reliable.\"\n\n\"I'm working on a project that requires fast matrix multiplication and I'm considering using Triton. Does anyone have a gemm kernel that I can use as a starting point?\"\n\n\"Is there a standard Triton kernel for gemm that I can reference or use directly in my project?\"\n\n\"I'm trying to accelerate some linear algebra operations using Triton and I need a high-performance gemm kernel. Are there any existing implementations I can leverage?\"",
    "operation": "gemm",
    "tier": 1,
    "reference_op": "torch.matmul",
    "input_shapes": [
      [1024, 128],
      [128, 128]
    ],
    "output_shape": [1024, 128]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my model's inference performance and I was wondering if there's a Triton kernel available that implements softmax? I've been searching through the docs but couldn't find anything obvious.\"\n\n\"Can someone point me to a Triton kernel that does softmax? I'm looking to accelerate my deep learning model's inference and I think Triton could be a good fit.\"\n\n\"Is there an existing Triton implementation of the softmax operation that I can use? I'd rather not have to write my own if it's already been done.\"\n\n\"I'm looking for a high-performance softmax implementation in Triton. Does anyone know if one exists? I'd love to avoid reinventing the wheel here.\"\n\n\"Do you have a Triton kernel that implements softmax? We're evaluating different options for accelerating our model's inference and Triton is one of the ones on our radar.\"",
    "operation": "softmax",
    "tier": 1,
    "reference_op": "torch.softmax",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my model's inference performance and I was wondering if there's a Triton kernel available that can handle softmax calculations efficiently?\"\n\n\"Can someone point me to a Triton implementation of the softmax function? I'm looking to integrate it into my deep learning pipeline.\"\n\n\"I'm in the process of porting our ML model to use Triton for acceleration, and I need a softmax kernel. Does anyone have a pre-existing implementation I can leverage?\"\n\n\"Is there a standard Triton kernel for softmax that I can use as a reference? I'm trying to get a better understanding of how to structure my own custom kernels.\"\n\n\"Do you know if Triton has a built-in softmax operation or if I need to write my own? If so, can you share an example or point me to the relevant documentation?\"",
    "operation": "softmax",
    "tier": 1,
    "reference_op": "torch.softmax",
    "input_shapes": [[256]],
    "output_shape": [256]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model's performance and I was wondering if there's a Triton kernel available that does a fused softmax operation - that would really help me out. Do you know if anyone has implemented something like that?\"",
    "operation": "fused_softmax",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model's performance and I was wondering if there's a Triton kernel available that implements a fused softmax operation? That would really help me out.\"\n\n\"Can someone point me to a Triton kernel that does a fused softmax? I'm looking to speed up my deep learning model's training time and I think this could be a key optimization.\"\n\n\"I'm working on a project that requires highly optimized softmax calculations. Does anyone know if there's a pre-existing Triton kernel that implements fused softmax, or would I need to write my own?\"\n\n\"Is there a Triton kernel available for fused softmax? I'm trying to reduce memory bandwidth usage in my model and I think a fused kernel could make a big difference.\"\n\n\"Hi, I'm looking for a Triton kernel that implements the softmax operation in a fused manner, i.e., without having to materialize the intermediate results. Does anyone have a pointer to such a kernel or some guidance on how to implement it?\"",
    "operation": "fused_softmax",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[128]],
    "output_shape": [128]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my matrix multiplication code and I heard Triton can do some really efficient block-level GEMM operations. Can someone point me to a kernel that does that? Or maybe give me some tips on how to write one myself?\"",
    "operation": "block_gemm",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[1536]],
    "output_shape": [1536]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my matrix multiplication code and I was wondering if Triton has a kernel that can handle blocked GEMM operations? Specifically, I'm looking for something that can efficiently perform batched matrix multiplication with a relatively small batch size.\"",
    "operation": "block_gemm",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[1536]],
    "output_shape": [1536]
  },
  {
    "prompt": "\"Hey, I'm working on a project that requires summing up a stream of numbers in real-time. Can someone point me to a Triton kernel that implements an online sum, or should I roll my own?\"",
    "operation": "online_sum",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[128]],
    "output_shape": [128]
  },
  {
    "prompt": "\"Hey, I'm working on a project that requires a custom kernel for cumulative sum operations. Can someone point me to a Triton kernel that does online sum, or guide me on how to implement one if it doesn't exist?\"",
    "operation": "online_sum",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[4096]],
    "output_shape": [4096]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my matrix operations and I need a Triton kernel that can efficiently transpose a large matrix using tiling. Do you have an example implementation or know how I could go about writing one?\"\n\n\"Can someone provide a Triton kernel that performs a tiled matrix transpose? I'm looking for a high-performance solution and would appreciate any guidance on how to implement it.\"\n\n\"I'm working on a project that requires fast matrix transposition and I've heard Triton is a good choice. Specifically, I need a kernel that uses tiling to minimize memory accesses. Does anyone have a Triton kernel that does this?\"\n\n\"How would I go about writing a Triton kernel for tiled matrix transpose? Are there any existing examples or documentation that I could draw upon?\"\n\n\"I'm looking for a Triton kernel that can transpose a matrix in a tiled manner to reduce memory bandwidth usage. Can anyone point me in the right direction or provide an example implementation?\"",
    "operation": "tiled_transpose",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm working on optimizing some matrix operations and I was wondering if there's a Triton kernel out there that does a tiled transpose? Something that can handle large matrices efficiently would be great.\"\n\n\"Can someone provide a Triton kernel implementation for a tiled matrix transpose operation? I'm looking for something that minimizes memory accesses and is highly performant.\"\n\n\"I'm trying to speed up my deep learning model and I think a tiled transpose kernel in Triton could be the way to go. Does anyone have an example of how to implement this?\"\n\n\"Is there an existing Triton kernel that performs a tiled transpose? I'd like to avoid reinventing the wheel if possible.\"\n\n\"For a project I'm working on, I need a high-performance tiled transpose operation. Can Triton handle this? If so, what's the best way to implement it?\"",
    "operation": "tiled_transpose",
    "tier": 2,
    "reference_op": "unknown",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of softmax calculations and I was wondering if there's a Triton kernel out there that can handle online softmax for me? I've been digging through the Triton docs but couldn't find anything obvious.\"\n\n\"Can someone point me to a Triton implementation of online softmax? I'm trying to optimize my model's performance and I think this could be a bottleneck.\"\n\n\"I'm looking for a high-performance softmax implementation in Triton that can handle streaming data. Does anyone know of a kernel that can do online softmax?\"\n\n\"Is there a Triton kernel available that performs online softmax? I'd love to avoid reimplementing the wheel if it's already out there.\"\n\n\"Hi, does Triton have a built-in or example kernel for online softmax? I'm trying to get up to speed with Triton and this would be super helpful.\"",
    "operation": "online_softmax",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[4096]],
    "output_shape": [4096]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of softmax calculations and I was wondering if there's a Triton kernel out there that can handle online softmax for me? Something that's optimized for performance would be great.\"\n\n\"Can someone point me to a Triton implementation of online softmax? I'm trying to avoid reinventing the wheel here and a pre-existing kernel would save me a ton of time.\"\n\n\"I'm looking for a high-performance softmax kernel that can be used in a streaming context. Does anyone know if there's a Triton kernel that implements online softmax?\"\n\n\"Is there a Triton kernel available that performs online softmax? I'd love to get my hands on something that's already been optimized for this use case.\"\n\n\"Do you know of any Triton kernels that can compute softmax in an online fashion? I'm trying to implement a model that requires this and I'd love to leverage Triton's performance capabilities.\"",
    "operation": "online_softmax",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[1024]],
    "output_shape": [1024]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model's performance and I need a Triton kernel that implements blocked attention - can you point me to one or help me write it?\"",
    "operation": "block_attention",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[1024]],
    "output_shape": [1024]
  },
  {
    "prompt": "\"Hey, I'm working on optimizing our transformer model and I need a Triton kernel that implements blocked attention - can someone point me to an example or help me write one?\"\n\n\"Can I get a Triton kernel implementation for blocked attention? We're trying to reduce memory access patterns in our transformer and this seems like a good starting point.\"\n\n\"I'm trying to integrate blocked attention into our existing ML pipeline using Triton. Does anyone have a kernel I can use as a reference or should I start from scratch?\"\n\n\"For our research project, we need to implement blocked attention using Triton kernels. Could someone provide an example or guide on how to achieve this?\"\n\n\"Does Triton have a built-in kernel or example for blocked attention that I can leverage for my transformer model, or will I need to create a custom kernel?\"",
    "operation": "block_attention",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[4096]],
    "output_shape": [4096]
  },
  {
    "prompt": "\"Hey, I'm working on a project that requires some heavy matrix multiplications and I was wondering if there's a Triton kernel available that implements an optimized GEMM operation? I've been looking through the Triton docs but couldn't find anything straightforward. Would be great if someone could point me in the right direction or provide an example.\"",
    "operation": "optimized_gemm",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[512]],
    "output_shape": [512]
  },
  {
    "prompt": "\"Hey, I'm working on a project that involves a lot of matrix multiplications and I was wondering if there's a Triton kernel available that implements an optimized GEMM operation? I've been trying to get the most out of our GPU and I heard Triton can give some great performance boosts, but I'm not sure where to start looking for something like this. Is there a standard or commonly used kernel for this kind of thing?\"",
    "operation": "optimized_gemm",
    "tier": 3,
    "reference_op": "unknown",
    "input_shapes": [[1536]],
    "output_shape": [1536]
  },
  {
    "prompt": "Hey, I'm trying to optimize my transformer model and I need a Triton kernel that implements standard self-attention - can you point me to one that does vanilla attention? \n\nDo you have a Triton kernel implementation for the basic attention mechanism used in transformers? \n\nI'm looking for a Triton kernel that performs the typical attention calculation - Q dot K transpose, scale, softmax, then matmul with V. Is there a straightforward implementation available?\n\nCan someone provide a Triton kernel that computes attention scores using the standard formula? \n\nFor a project, I need to integrate a Triton kernel that does the usual self-attention computation. Does anyone have a simple implementation they can share?",
    "operation": "vanilla_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[1024]],
    "output_shape": [1024]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model's attention mechanism and I was wondering if there's a Triton kernel available that implements standard vanilla attention?\"\n\n\"Can someone point me to a Triton implementation of the basic attention mechanism, you know, the one that's just a straightforward softmax(Q*K^T/sqrt(d))*V calculation?\"\n\n\"I need a highly optimized attention kernel for my research project. Does anyone know if there's a Triton kernel that performs vanilla attention, i.e., the scaled dot-product attention without any additional bells or whistles?\"\n\n\"Is there a Triton kernel out there that does good ol' fashioned attention, the kind that's just Q, K, and V matrices going into a softmax and then getting multiplied by V?\"\n\n\"For a project I'm working on, I require an efficient implementation of the vanilla attention mechanism. Would anyone happen to know if a Triton kernel for this exists, or would I need to write one from scratch?\"",
    "operation": "vanilla_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[2048]],
    "output_shape": [2048]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model and I heard Triton kernels can give a nice speedup. Does anyone have a Triton kernel implementation of flash attention that I could use?\"\n\n\"I'm looking for a high-performance attention mechanism for my deep learning project. Can someone point me to a Triton kernel that implements flash attention?\"\n\n\"Do you know if there's a Triton kernel available that does flash attention? I'm trying to get my model training faster and I've heard that's a key optimization.\"\n\n\"I'm trying to implement a transformer model using Triton for the kernel. Specifically, I need a kernel that does flash attention - does anyone have an example or a pointer to an existing implementation?\"\n\n\"Is there a Triton kernel out there that implements the flash attention algorithm? I'd love to get my hands on it and see if it speeds up my model.\"",
    "operation": "flash_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model and I was wondering if there's a Triton kernel available that implements flash attention? I've been looking everywhere but couldn't find one, would really appreciate a pointer or a simple example if someone's already written it.\"",
    "operation": "flash_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[1024]],
    "output_shape": [1024]
  },
  {
    "prompt": "\"Hey, I'm trying to optimize my transformer model and I was wondering if there's a Triton kernel available that implements multi-head attention - something that could give me a good starting point or maybe even be production-ready?\"",
    "operation": "multi_head_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[768]],
    "output_shape": [768]
  },
  {
    "prompt": "\"Hey, I'm working on a transformer model and I need to implement multi-head attention. Does Triton have a kernel for that? If so, can you point me to it or tell me how to write one?\"\n\n\"Can someone provide a Triton kernel implementation for multi-head attention? I'm having trouble figuring out how to optimize it for our specific use case.\"\n\n\"I'm trying to accelerate my transformer model using Triton. Is there an existing kernel that performs multi-head attention I can leverage?\"\n\n\"How would I go about writing a Triton kernel for multi-head attention? Are there any examples or templates I can use as a starting point?\"\n\n\"Does anyone have a Triton kernel for multi-head attention lying around? I'd love to take a look and see how it's implemented.\"",
    "operation": "multi_head_attention",
    "tier": 4,
    "reference_op": "unknown",
    "input_shapes": [[256]],
    "output_shape": [256]
  }
]

