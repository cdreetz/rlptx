[
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes relu of a 1D tensor of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We get the offset of the elements our program is responsible for.\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    # Load x\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Compute ReLU\n    output = tl.maximum(x, 0)\n    # Store output\n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We get the offset of the elements our program is responsible for.\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    # Load x\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Compute ReLU\n    output = tl.maximum(x, 0)\n    # Store output\n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "relu",
    "input_shapes": [
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise divide of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x / y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x / y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "divide",
    "input_shapes": [
      [
        32768
      ],
      [
        32768
      ]
    ],
    "output_shape": [
      32768
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes sigmoid of a 1D tensor of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # * Pointer to input vector\n    output_ptr,  # * Pointer to output vector\n    N,  # * Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # * Block size for the kernel\n):\n    # Calculate the block index and the thread index within the block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data with masking to handle edge cases\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute sigmoid\n    sigmoid_x = 1 / (1 + tl.exp(-x))\n\n    # Store the result\n    tl.store(output_ptr + offsets, sigmoid_x, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # * Pointer to input vector\n    output_ptr,  # * Pointer to output vector\n    N,  # * Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # * Block size for the kernel\n):\n    # Calculate the block index and the thread index within the block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data with masking to handle edge cases\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute sigmoid\n    sigmoid_x = 1 / (1 + tl.exp(-x))\n\n    # Store the result\n    tl.store(output_ptr + offsets, sigmoid_x, mask=mask)",
    "operation": "sigmoid",
    "input_shapes": [
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes relu of a 1D tensor of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" to cover the entire input tensor.\n    # Triton will automatically generate and execute a kernel for each \"program\".\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid, so axis is 0.\n    # This program will process inputs that are offset from the initial data.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to guard memory operations against out-of-bounds accesses.\n    mask = offsets < N\n    # Load x at the given `offsets`\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Apply ReLU\n    output = tl.maximum(x, 0)\n    # Write output back to memory\n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" to cover the entire input tensor.\n    # Triton will automatically generate and execute a kernel for each \"program\".\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid, so axis is 0.\n    # This program will process inputs that are offset from the initial data.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to guard memory operations against out-of-bounds accesses.\n    mask = offsets < N\n    # Load x at the given `offsets`\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Apply ReLU\n    output = tl.maximum(x, 0)\n    # Write output back to memory\n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "relu",
    "input_shapes": [
      [
        4096
      ]
    ],
    "output_shape": [
      4096
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise divide of two 1D tensors of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Avoid division by zero\n    y = tl.where(y == 0, tl.full_like(y, float('nan')), y)\n    \n    output = x / y\n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Avoid division by zero\n    y = tl.where(y == 0, tl.full_like(y, float('nan')), y)\n    \n    output = x / y\n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "divide",
    "input_shapes": [
      [
        8192
      ],
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise divide of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x / y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x / y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "divide",
    "input_shapes": [
      [
        16384
      ],
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes sigmoid of a 1D tensor of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make sure that we don't go out of bounds\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    # Load x\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # Compute sigmoid\n    sigmoid = 1 / (1 + tl.exp(-x))\n    \n    # Store the result\n    tl.store(output_ptr + offsets, sigmoid, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make sure that we don't go out of bounds\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    # Load x\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # Compute sigmoid\n    sigmoid = 1 / (1 + tl.exp(-x))\n    \n    # Store the result\n    tl.store(output_ptr + offsets, sigmoid, mask=mask)",
    "operation": "sigmoid",
    "input_shapes": [
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise multiply of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef elementwise_multiply(x, y):\n    N = x.numel()\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    num_blocks = triton.cdiv(N, BLOCK_SIZE)\n    output = torch.empty_like(x)\n    \n    elementwise_multiply_kernel[(num_blocks,)](x, y, output, N, BLOCK_SIZE)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef elementwise_multiply(x, y):\n    N = x.numel()\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    num_blocks = triton.cdiv(N, BLOCK_SIZE)\n    output = torch.empty_like(x)\n    \n    elementwise_multiply_kernel[(num_blocks,)](x, y, output, N, BLOCK_SIZE)\n    return output",
    "operation": "multiply",
    "input_shapes": [
      [
        32768
      ],
      [
        32768
      ]
    ],
    "output_shape": [
      32768
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes relu of a 1D tensor of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" for the Triton runtime to run in parallel, \n    # so we get the ID of the current program and the total number of programs.\n    pid = tl.program_id(axis=0)\n    # Calculate the starting index for this program.\n    block_start = pid * BLOCK_SIZE\n    # Create a mask to handle edge cases where N is not a multiple of BLOCK_SIZE.\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    # Load x from memory.\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Apply ReLU.\n    output = tl.maximum(x, 0)\n    # Store the result in memory.\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef relu(x):\n    output = torch.empty_like(x)\n    N = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" for the Triton runtime to run in parallel, \n    # so we get the ID of the current program and the total number of programs.\n    pid = tl.program_id(axis=0)\n    # Calculate the starting index for this program.\n    block_start = pid * BLOCK_SIZE\n    # Create a mask to handle edge cases where N is not a multiple of BLOCK_SIZE.\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    # Load x from memory.\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Apply ReLU.\n    output = tl.maximum(x, 0)\n    # Store the result in memory.\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef relu(x):\n    output = torch.empty_like(x)\n    N = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output",
    "operation": "relu",
    "input_shapes": [
      [
        32768
      ]
    ],
    "output_shape": [
      32768
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise add of two 1D tensors of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "add",
    "input_shapes": [
      [
        1024
      ],
      [
        1024
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise multiply of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef elementwise_multiply(x, y):\n    N = x.shape[0]\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    output = torch.empty_like(x)\n    \n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    elementwise_multiply_kernel[grid](x, y, output, N, BLOCK_SIZE)\n    \n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef elementwise_multiply(x, y):\n    N = x.shape[0]\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    output = torch.empty_like(x)\n    \n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    elementwise_multiply_kernel[grid](x, y, output, N, BLOCK_SIZE)\n    \n    return output",
    "operation": "multiply",
    "input_shapes": [
      [
        16384
      ],
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes sigmoid of a 1D tensor of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Compute the block index and the offset within the block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data with masking to handle edge cases\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute sigmoid\n    sigmoid_x = 1 / (1 + tl.exp(-x))\n\n    # Store the result with masking\n    tl.store(output_ptr + offsets, sigmoid_x, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Compute the block index and the offset within the block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data with masking to handle edge cases\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute sigmoid\n    sigmoid_x = 1 / (1 + tl.exp(-x))\n\n    # Store the result with masking\n    tl.store(output_ptr + offsets, sigmoid_x, mask=mask)",
    "operation": "sigmoid",
    "input_shapes": [
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise multiply of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef element_wise_multiply(x, y):\n    N = x.shape[0]\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    output = torch.empty_like(x)\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    element_wise_multiply_kernel[grid](x, y, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_multiply_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef element_wise_multiply(x, y):\n    N = x.shape[0]\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    output = torch.empty_like(x)\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    element_wise_multiply_kernel[grid](x, y, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output",
    "operation": "multiply",
    "input_shapes": [
      [
        8192
      ],
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes relu of a 1D tensor of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" for the Triton runtime to run in parallel, \n    # so we get the \"program id\" of the current program\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the offset of the first element this program will process\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a mask to handle cases where N is not a multiple of BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    # Load x from memory with the calculated offsets and mask\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # Compute ReLU\n    output = tl.maximum(x, 0)\n    \n    # Store the result in memory\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef relu(x):\n    output = torch.empty_like(x)\n    N = x.numel()\n    BLOCK_SIZE = 1024  # You can tune this for your specific hardware\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" for the Triton runtime to run in parallel, \n    # so we get the \"program id\" of the current program\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the offset of the first element this program will process\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a mask to handle cases where N is not a multiple of BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    # Load x from memory with the calculated offsets and mask\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # Compute ReLU\n    output = tl.maximum(x, 0)\n    \n    # Store the result in memory\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef relu(x):\n    output = torch.empty_like(x)\n    N = x.numel()\n    BLOCK_SIZE = 1024  # You can tune this for your specific hardware\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, output, N, BLOCK_SIZE=BLOCK_SIZE)\n    return output",
    "operation": "relu",
    "input_shapes": [
      [
        4096
      ]
    ],
    "output_shape": [
      4096
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes sigmoid of a 1D tensor of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" to cover the entire input tensor.\n    # Triton will automatically parallelize these programs across available SMs.\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    # This program will process inputs that are offset from the initial data pointer.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = 1 / (1 + tl.exp(-x))\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef sigmoid(x):\n    N = x.numel()\n    output = torch.empty_like(x)\n    # We need to grid launch this kernel to cover the entire input tensor.\n    # For a 1D tensor, we use a 1D grid where each program processes BLOCK_SIZE elements.\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    sigmoid_kernel[grid](x, output, N, BLOCK_SIZE=1024)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sigmoid_kernel(\n    x_ptr,  # *Pointer* to input vector\n    output_ptr,  # *Pointer* to output vector\n    N,  # Size of the vector\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n):\n    # We need to make multiple \"programs\" to cover the entire input tensor.\n    # Triton will automatically parallelize these programs across available SMs.\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    # This program will process inputs that are offset from the initial data pointer.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = 1 / (1 + tl.exp(-x))\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef sigmoid(x):\n    N = x.numel()\n    output = torch.empty_like(x)\n    # We need to grid launch this kernel to cover the entire input tensor.\n    # For a 1D tensor, we use a 1D grid where each program processes BLOCK_SIZE elements.\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    sigmoid_kernel[grid](x, output, N, BLOCK_SIZE=1024)\n    return output",
    "operation": "sigmoid",
    "input_shapes": [
      [
        8192
      ]
    ],
    "output_shape": [
      8192
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise subtract of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "subtract",
    "input_shapes": [
      [
        16384
      ],
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise subtract of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "subtract",
    "input_shapes": [
      [
        4096
      ],
      [
        4096
      ]
    ],
    "output_shape": [
      4096
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise divide of two 1D tensors of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Avoid division by zero\n    y = tl.where(y == 0, tl.full_like(y, float('nan')), y)\n    \n    output = x / y\n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef element_wise_divide_kernel(\n    x_ptr, y_ptr, output_ptr, \n    N: tl.constexpr, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Avoid division by zero\n    y = tl.where(y == 0, tl.full_like(y, float('nan')), y)\n    \n    output = x / y\n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "divide",
    "input_shapes": [
      [
        32768
      ],
      [
        32768
      ]
    ],
    "output_shape": [
      32768
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise subtract of two 1D tensors of size N\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef subtract_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x - y\n    \n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "subtract",
    "input_shapes": [
      [
        4096
      ],
      [
        4096
      ]
    ],
    "output_shape": [
      4096
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that performs element-wise add of two 1D tensors of size N with efficient memory access patterns\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    x_ptr, \n    y_ptr, \n    output_ptr, \n    N, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    \n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)",
    "operation": "add",
    "input_shapes": [
      [
        16384
      ],
      [
        16384
      ]
    ],
    "output_shape": [
      16384
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the mean reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # input tensor\n    Y_ptr,  # output tensor\n    M,      # number of rows in X\n    N,      # number of columns in X\n    stride_xm,  # stride of X along M dimension\n    stride_xn,  # stride of X along N dimension\n    stride_ym,  # stride of Y along M dimension\n    BLOCK_SIZE_N: tl.constexpr,  # number of elements to reduce at once\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    # Calculate the offset for the current row\n    row_offset = row_idx * stride_xm\n\n    # Calculate the start and end column indices for the current block\n    col_start = col_block_idx * BLOCK_SIZE_N\n    col_end = min(col_start + BLOCK_SIZE_N, N)\n\n    # Initialize accumulator\n    accumulator = 0.0\n\n    # Load and accumulate elements from the input tensor\n    for col_idx in range(col_start, col_end):\n        col_offset = col_idx * stride_xn\n        x = tl.load(X_ptr + row_offset + col_offset)\n        accumulator += x\n\n    # Store the mean of the accumulated elements to the output tensor\n    y = accumulator / N\n    tl.store(Y_ptr + row_idx * stride_ym, y)\n\n# Example usage (not part of the kernel code)\n# M, N = 1024, 1024\n# BLOCK_SIZE_N = 256\n# grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n# mean_reduction_kernel[grid](X_ptr, Y_ptr, M, N, X.stride(0), X.stride(1), Y.stride(0), BLOCK_SIZE_N=BLOCK_SIZE_N)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # input tensor\n    Y_ptr,  # output tensor\n    M,      # number of rows in X\n    N,      # number of columns in X\n    stride_xm,  # stride of X along M dimension\n    stride_xn,  # stride of X along N dimension\n    stride_ym,  # stride of Y along M dimension\n    BLOCK_SIZE_N: tl.constexpr,  # number of elements to reduce at once\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    # Calculate the offset for the current row\n    row_offset = row_idx * stride_xm\n\n    # Calculate the start and end column indices for the current block\n    col_start = col_block_idx * BLOCK_SIZE_N\n    col_end = min(col_start + BLOCK_SIZE_N, N)\n\n    # Initialize accumulator\n    accumulator = 0.0\n\n    # Load and accumulate elements from the input tensor\n    for col_idx in range(col_start, col_end):\n        col_offset = col_idx * stride_xn\n        x = tl.load(X_ptr + row_offset + col_offset)\n        accumulator += x\n\n    # Store the mean of the accumulated elements to the output tensor\n    y = accumulator / N\n    tl.store(Y_ptr + row_idx * stride_ym, y)\n\n# Example usage (not part of the kernel code)\n# M, N = 1024, 1024\n# BLOCK_SIZE_N = 256\n# grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n# mean_reduction_kernel[grid](X_ptr, Y_ptr, M, N, X.stride(0), X.stride(1), Y.stride(0), BLOCK_SIZE_N=BLOCK_SIZE_N)",
    "operation": "mean_reduction",
    "input_shapes": [
      [
        256,
        256
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the mean reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # input tensor\n    Y_ptr,  # output tensor\n    M,      # number of rows in X\n    N,      # number of columns in X\n    stride_xm,  # stride of X along M\n    stride_xn,  # stride of X along N\n    stride_ym,  # stride of Y along M\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    # program id and number of programs\n    pid_m = tl.program_id(axis=0)\n    # compute offset in M dimension\n    offs_m = pid_m\n    \n    # compute offset in N dimension\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    \n    # load data from X\n    X_ptrs = X_ptr + offs_m * stride_xm + offs_n * stride_xn\n    mask = offs_n < N\n    x = tl.load(X_ptrs, mask=mask, other=0.0)\n    \n    # compute mean\n    mean = tl.sum(x, axis=0) / N\n    \n    # store result in Y\n    Y_ptr = Y_ptr + offs_m * stride_ym\n    tl.store(Y_ptr, mean)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, device=X.device, dtype=torch.float32)\n    \n    # define grid and BLOCK_SIZE\n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 1024\n    \n    # launch kernel\n    mean_reduction_kernel[grid](\n        X, Y, M, N, \n        X.stride(0), X.stride(1), \n        Y.stride(0), \n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n    )\n    return Y\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # input tensor\n    Y_ptr,  # output tensor\n    M,      # number of rows in X\n    N,      # number of columns in X\n    stride_xm,  # stride of X along M\n    stride_xn,  # stride of X along N\n    stride_ym,  # stride of Y along M\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    # program id and number of programs\n    pid_m = tl.program_id(axis=0)\n    # compute offset in M dimension\n    offs_m = pid_m\n    \n    # compute offset in N dimension\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    \n    # load data from X\n    X_ptrs = X_ptr + offs_m * stride_xm + offs_n * stride_xn\n    mask = offs_n < N\n    x = tl.load(X_ptrs, mask=mask, other=0.0)\n    \n    # compute mean\n    mean = tl.sum(x, axis=0) / N\n    \n    # store result in Y\n    Y_ptr = Y_ptr + offs_m * stride_ym\n    tl.store(Y_ptr, mean)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, device=X.device, dtype=torch.float32)\n    \n    # define grid and BLOCK_SIZE\n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 1024\n    \n    # launch kernel\n    mean_reduction_kernel[grid](\n        X, Y, M, N, \n        X.stride(0), X.stride(1), \n        Y.stride(0), \n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n    )\n    return Y",
    "operation": "mean_reduction",
    "input_shapes": [
      [
        512,
        256
      ]
    ],
    "output_shape": [
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the max reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n    \n    for n_offset in range(BLOCK_SIZE_N, N, BLOCK_SIZE_N):\n        offs_n = n_offset + tl.arange(0, BLOCK_SIZE_N)\n        val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    tl.store(output_ptr, tl.max(max_val, axis=0))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n    \n    for n_offset in range(BLOCK_SIZE_N, N, BLOCK_SIZE_N):\n        offs_n = n_offset + tl.arange(0, BLOCK_SIZE_N)\n        val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    tl.store(output_ptr, tl.max(max_val, axis=0))",
    "operation": "max_reduction",
    "input_shapes": [
      [
        128,
        1024
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the min reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M, N, \n    stride_M, stride_N,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    mask_m = offs_m < M\n\n    min_vals = tl.full([BLOCK_SIZE_M], float('inf'), dtype=tl.float32)\n\n    for n in range(0, N, BLOCK_SIZE_N):\n        mask_n = offs_n + n < N\n        mask = mask_m[:, None] & mask_n[None, :]\n        input = tl.load(input_ptr + offs_m[:, None] * stride_M + (offs_n[None, :] + n) * stride_N, mask=mask, other=float('inf'))\n        min_vals = tl.minimum(min_vals, tl.min(input, axis=1))\n\n    tl.store(output_ptr + offs_m * stride_M, min_vals, mask=mask_m)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M, N, \n    stride_M, stride_N,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    mask_m = offs_m < M\n\n    min_vals = tl.full([BLOCK_SIZE_M], float('inf'), dtype=tl.float32)\n\n    for n in range(0, N, BLOCK_SIZE_N):\n        mask_n = offs_n + n < N\n        mask = mask_m[:, None] & mask_n[None, :]\n        input = tl.load(input_ptr + offs_m[:, None] * stride_M + (offs_n[None, :] + n) * stride_N, mask=mask, other=float('inf'))\n        min_vals = tl.minimum(min_vals, tl.min(input, axis=1))\n\n    tl.store(output_ptr + offs_m * stride_M, min_vals, mask=mask_m)",
    "operation": "min_reduction",
    "input_shapes": [
      [
        256,
        512
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the min reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M, N, \n    stride_M, stride_N,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * stride_M\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = offs_n < N\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        \n        offs_n += BLOCK_SIZE_N\n        input_ptr += BLOCK_SIZE_N * stride_N\n    \n    min_val = tl.min(min_val, axis=0)\n    tl.store(output_ptr, min_val)\n\ndef min_reduction(x):\n    M, N = x.shape\n    output = torch.empty(M, device=x.device, dtype=x.dtype)\n    \n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 1024\n    min_reduction_kernel[grid](\n        x, output, \n        M, N, \n        x.stride(0), x.stride(1),\n        BLOCK_SIZE_M=M, BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M, N, \n    stride_M, stride_N,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * stride_M\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = offs_n < N\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        \n        offs_n += BLOCK_SIZE_N\n        input_ptr += BLOCK_SIZE_N * stride_N\n    \n    min_val = tl.min(min_val, axis=0)\n    tl.store(output_ptr, min_val)\n\ndef min_reduction(x):\n    M, N = x.shape\n    output = torch.empty(M, device=x.device, dtype=x.dtype)\n    \n    grid = lambda meta: (M, )\n    BLOCK_SIZE_N = 1024\n    min_reduction_kernel[grid](\n        x, output, \n        M, N, \n        x.stride(0), x.stride(1),\n        BLOCK_SIZE_M=M, BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return output",
    "operation": "min_reduction",
    "input_shapes": [
      [
        256,
        128
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    input_ptr = input_ptr + row_idx * N + col_offsets\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    accumulator = tl.where(col_offsets < N, tl.load(input_ptr), 0.0)\n    for _ in range(1):\n        tl.store(output_ptr + row_idx, tl.sum(accumulator, axis=0))\n\n    # for larger BLOCK_SIZE_N we need to do a proper reduction\n    if BLOCK_SIZE_N > 1:\n        accumulator = tl.sum(accumulator, axis=0)\n        tl.store(output_ptr + row_idx, accumulator)\n\n# for larger N, we need to do a multi-block reduction\n@triton.jit\ndef sum_reduction_kernel_large_N(\n    input_ptr, \n    output_ptr, \n    partial_sums_ptr,\n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_block_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    row_offsets = row_block_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    mask = (row_offsets[:, None] < M) & (col_offsets[None, :] < N)\n    input_ptr = input_ptr + row_offsets[:, None] * N + col_offsets[None, :]\n    partial_sums_ptr = partial_sums_ptr + row_offsets * (N // BLOCK_SIZE_N + 1) + col_block_idx\n\n    accumulator = tl.sum(tl.where(mask, tl.load(input_ptr), 0.0), axis=1)\n    tl.store(partial_sums_ptr, accumulator)\n\n@triton.jit\ndef sum_reduction_kernel_final(\n    partial_sums_ptr,\n    output_ptr,\n    M,\n    N,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_M: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_offsets = row_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    mask = row_offsets < M\n    row_offsets = tl.where(mask, row_offsets, M-1)\n    partial_sums_ptr = partial_sums_ptr + row_offsets * (N // BLOCK_SIZE_N + 1)\n    accumulator = tl.load(partial_sums_ptr, mask=mask)\n    for i in range(1, N // BLOCK_SIZE_N + 1):\n        partial_sums_ptr = partial_sums_ptr + 1\n        accumulator += tl.load(partial_sums_ptr, mask=mask)\n    tl.store(output_ptr + row_offsets, accumulator, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    input_ptr = input_ptr + row_idx * N + col_offsets\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    accumulator = tl.where(col_offsets < N, tl.load(input_ptr), 0.0)\n    for _ in range(1):\n        tl.store(output_ptr + row_idx, tl.sum(accumulator, axis=0))\n\n    # for larger BLOCK_SIZE_N we need to do a proper reduction\n    if BLOCK_SIZE_N > 1:\n        accumulator = tl.sum(accumulator, axis=0)\n        tl.store(output_ptr + row_idx, accumulator)\n\n# for larger N, we need to do a multi-block reduction\n@triton.jit\ndef sum_reduction_kernel_large_N(\n    input_ptr, \n    output_ptr, \n    partial_sums_ptr,\n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_block_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    row_offsets = row_block_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    mask = (row_offsets[:, None] < M) & (col_offsets[None, :] < N)\n    input_ptr = input_ptr + row_offsets[:, None] * N + col_offsets[None, :]\n    partial_sums_ptr = partial_sums_ptr + row_offsets * (N // BLOCK_SIZE_N + 1) + col_block_idx\n\n    accumulator = tl.sum(tl.where(mask, tl.load(input_ptr), 0.0), axis=1)\n    tl.store(partial_sums_ptr, accumulator)\n\n@triton.jit\ndef sum_reduction_kernel_final(\n    partial_sums_ptr,\n    output_ptr,\n    M,\n    N,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_M: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_offsets = row_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    mask = row_offsets < M\n    row_offsets = tl.where(mask, row_offsets, M-1)\n    partial_sums_ptr = partial_sums_ptr + row_offsets * (N // BLOCK_SIZE_N + 1)\n    accumulator = tl.load(partial_sums_ptr, mask=mask)\n    for i in range(1, N // BLOCK_SIZE_N + 1):\n        partial_sums_ptr = partial_sums_ptr + 1\n        accumulator += tl.load(partial_sums_ptr, mask=mask)\n    tl.store(output_ptr + row_offsets, accumulator, mask=mask)",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        256,
        1024
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    offs_n = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    input_ptrs = input_ptr + row_idx * N + offs_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = offs_n < N\n        input_vals = tl.load(input_ptrs, mask=mask, other=0.0)\n        accumulator += input_vals\n        offs_n += BLOCK_SIZE_N\n        input_ptrs += BLOCK_SIZE_N\n\n    accumulator = tl.sum(accumulator, axis=0)\n    output_ptr = output_ptr + row_idx\n    tl.store(output_ptr, accumulator)\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    output = torch.empty(M, dtype=x.dtype, device=x.device)\n\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    offs_n = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    input_ptrs = input_ptr + row_idx * N + offs_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = offs_n < N\n        input_vals = tl.load(input_ptrs, mask=mask, other=0.0)\n        accumulator += input_vals\n        offs_n += BLOCK_SIZE_N\n        input_ptrs += BLOCK_SIZE_N\n\n    accumulator = tl.sum(accumulator, axis=0)\n    output_ptr = output_ptr + row_idx\n    tl.store(output_ptr, accumulator)\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    output = torch.empty(M, dtype=x.dtype, device=x.device)\n\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        512,
        1024
      ]
    ],
    "output_shape": [
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the max reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.full([BLOCK_SIZE_N], -float('inf'), dtype=tl.float32)\n    for start_n in range(0, N, BLOCK_SIZE_N):\n        mask = start_n + offs_n < N\n        val = tl.load(input_ptr + start_n + offs_n, mask=mask, other=-float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    output_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, output_val)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.full([BLOCK_SIZE_N], -float('inf'), dtype=tl.float32)\n    for start_n in range(0, N, BLOCK_SIZE_N):\n        mask = start_n + offs_n < N\n        val = tl.load(input_ptr + start_n + offs_n, mask=mask, other=-float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    output_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, output_val)",
    "operation": "max_reduction",
    "input_shapes": [
      [
        128,
        256
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    \n    row_offs = row_idx\n    col_offs = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    input_ptr = input_ptr + row_offs * N\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offs < N\n        values = tl.load(input_ptr + col_offs, mask=mask, other=0.0)\n        accumulator += values\n        col_offs += BLOCK_SIZE_N\n    \n    output = tl.sum(accumulator, axis=0)\n    tl.store(output_ptr + row_idx, output)\n\ndef sum_reduction(input, BLOCK_SIZE_N=256):\n    M, N = input.shape\n    output = torch.empty(M, dtype=input.dtype, device=input.device)\n    grid = lambda meta: (M, triton.cdiv(N, meta['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](input, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    \n    row_offs = row_idx\n    col_offs = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    input_ptr = input_ptr + row_offs * N\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offs < N\n        values = tl.load(input_ptr + col_offs, mask=mask, other=0.0)\n        accumulator += values\n        col_offs += BLOCK_SIZE_N\n    \n    output = tl.sum(accumulator, axis=0)\n    tl.store(output_ptr + row_idx, output)\n\ndef sum_reduction(input, BLOCK_SIZE_N=256):\n    M, N = input.shape\n    output = torch.empty(M, dtype=input.dtype, device=input.device)\n    grid = lambda meta: (M, triton.cdiv(N, meta['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](input, output, M, N, BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return output",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        256,
        256
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    input_ptr = input_ptr + row_idx * N\n    output_ptr = output_ptr + row_idx\n\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = col_offsets < N\n\n    acc = tl.zeros((1,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        cols = tl.load(input_ptr + col_offsets, mask=mask, other=0.0)\n        acc += tl.sum(cols, axis=0)\n        col_offsets += BLOCK_SIZE_N\n\n    tl.store(output_ptr, acc[0])\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    output = torch.empty(M, device=x.device, dtype=torch.float32)\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    input_ptr = input_ptr + row_idx * N\n    output_ptr = output_ptr + row_idx\n\n    col_offsets = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = col_offsets < N\n\n    acc = tl.zeros((1,), dtype=tl.float32)\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        cols = tl.load(input_ptr + col_offsets, mask=mask, other=0.0)\n        acc += tl.sum(cols, axis=0)\n        col_offsets += BLOCK_SIZE_N\n\n    tl.store(output_ptr, acc[0])\n\ndef sum_reduction(x):\n    M, N = x.shape\n    BLOCK_SIZE_N = 256\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    output = torch.empty(M, device=x.device, dtype=torch.float32)\n    sum_reduction_kernel[grid](x, output, M, N, BLOCK_SIZE_N)\n    return output",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        128,
        256
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the max reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.full([BLOCK_SIZE_N], float('-inf'), dtype=tl.float32)\n    mask = offs_n < N\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('-inf'))\n        max_val = tl.maximum(max_val, val)\n        input_ptr += BLOCK_SIZE_N\n        offs_n += BLOCK_SIZE_N\n        mask = (offs_n < N) & (offs_n < (BLOCK_SIZE_N * (_ + 1)))\n    \n    max_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, max_val)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.full([BLOCK_SIZE_N], float('-inf'), dtype=tl.float32)\n    mask = offs_n < N\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('-inf'))\n        max_val = tl.maximum(max_val, val)\n        input_ptr += BLOCK_SIZE_N\n        offs_n += BLOCK_SIZE_N\n        mask = (offs_n < N) & (offs_n < (BLOCK_SIZE_N * (_ + 1)))\n    \n    max_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, max_val)",
    "operation": "max_reduction",
    "input_shapes": [
      [
        1024,
        512
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the mean reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X,  # input tensor\n    Y,  # output tensor\n    M,  # number of rows in X\n    N,  # number of columns in X\n    stride_xm,  # stride of X along M\n    stride_xn,  # stride of X along N\n    stride_y,  # stride of Y\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    col_offsets = col_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = col_offsets < N\n\n    X_ptrs = X + row * stride_xm + col_offsets * stride_xn\n    x = tl.load(X_ptrs, mask=mask, other=0.0)\n\n    accumulator = tl.sum(x, axis=0)\n\n    Y_ptr = Y + row * stride_y\n    tl.store(Y_ptr, accumulator / N)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, device=X.device, dtype=torch.float32)\n\n    BLOCK_SIZE_N = 1024\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n    mean_reduction_kernel[grid](X, Y, M, N, X.stride(0), X.stride(1), Y.stride(0), BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return Y\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X,  # input tensor\n    Y,  # output tensor\n    M,  # number of rows in X\n    N,  # number of columns in X\n    stride_xm,  # stride of X along M\n    stride_xn,  # stride of X along N\n    stride_y,  # stride of Y\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    col_offsets = col_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = col_offsets < N\n\n    X_ptrs = X + row * stride_xm + col_offsets * stride_xn\n    x = tl.load(X_ptrs, mask=mask, other=0.0)\n\n    accumulator = tl.sum(x, axis=0)\n\n    Y_ptr = Y + row * stride_y\n    tl.store(Y_ptr, accumulator / N)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, device=X.device, dtype=torch.float32)\n\n    BLOCK_SIZE_N = 1024\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_SIZE_N']))\n    mean_reduction_kernel[grid](X, Y, M, N, X.stride(0), X.stride(1), Y.stride(0), BLOCK_SIZE_N=BLOCK_SIZE_N)\n    return Y",
    "operation": "mean_reduction",
    "input_shapes": [
      [
        1024,
        512
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the min reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    stride_M, \n    stride_N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    output_row = tl.load(output_ptr + row)\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    min_val = float('inf')\n    for start_n in range(0, N, BLOCK_SIZE_N):\n        col = start_n + col_offsets\n        mask = col < N\n        val = tl.load(input_ptr + row * stride_M + col * stride_N, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, tl.min(val, axis=0))\n    tl.store(output_ptr + row, min_val)\n\ndef min_reduction(x):\n    M, N = x.shape\n    output = torch.empty(M, device=x.device, dtype=x.dtype)\n    grid = lambda meta: (M,)\n    BLOCK_SIZE_N = triton.next_power_of_2(N)\n    min_reduction_kernel[grid](\n        x, \n        output, \n        M, \n        N, \n        x.stride(0), \n        x.stride(1), \n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    stride_M, \n    stride_N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    output_row = tl.load(output_ptr + row)\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    min_val = float('inf')\n    for start_n in range(0, N, BLOCK_SIZE_N):\n        col = start_n + col_offsets\n        mask = col < N\n        val = tl.load(input_ptr + row * stride_M + col * stride_N, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, tl.min(val, axis=0))\n    tl.store(output_ptr + row, min_val)\n\ndef min_reduction(x):\n    M, N = x.shape\n    output = torch.empty(M, device=x.device, dtype=x.dtype)\n    grid = lambda meta: (M,)\n    BLOCK_SIZE_N = triton.next_power_of_2(N)\n    min_reduction_kernel[grid](\n        x, \n        output, \n        M, \n        N, \n        x.stride(0), \n        x.stride(1), \n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return output",
    "operation": "min_reduction",
    "input_shapes": [
      [
        256,
        512
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the mean reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # Pointer to input tensor\n    Y_ptr,  # Pointer to output tensor\n    M,      # Number of rows in input tensor\n    N,      # Number of columns in input tensor\n    stride_xm,  # Stride of X along M dimension\n    stride_xn,  # Stride of X along N dimension\n    stride_y,   # Stride of Y\n    BLOCK_SIZE_N: tl.constexpr,  # Block size along N dimension\n):\n    # Program ID\n    pid_m = tl.program_id(axis=0)\n\n    # Calculate offsets\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_m = pid_m * stride_xm + tl.zeros([BLOCK_SIZE_N], dtype=tl.int64)\n\n    # Initialize accumulator\n    acc = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n\n    # Compute mean reduction\n    for offs in range(0, N, BLOCK_SIZE_N):\n        mask = offs_n + offs < N\n        offs_x = offs_m + (offs + offs_n) * stride_xn\n        x = tl.load(X_ptr + offs_x, mask=mask, other=0.0)\n        acc += x\n\n    # Reduce accumulator\n    acc = tl.sum(acc, axis=0)\n\n    # Store result\n    y_ptr = Y_ptr + pid_m * stride_y\n    tl.store(y_ptr, acc / N)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, dtype=X.dtype, device=X.device)\n\n    # Define grid and block size\n    grid = lambda META: (M, )\n    BLOCK_SIZE_N = 256\n\n    # Launch kernel\n    mean_reduction_kernel[grid](\n        X, Y, M, N,\n        X.stride(0), X.stride(1), Y.stride(0),\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n    )\n    return Y\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr,  # Pointer to input tensor\n    Y_ptr,  # Pointer to output tensor\n    M,      # Number of rows in input tensor\n    N,      # Number of columns in input tensor\n    stride_xm,  # Stride of X along M dimension\n    stride_xn,  # Stride of X along N dimension\n    stride_y,   # Stride of Y\n    BLOCK_SIZE_N: tl.constexpr,  # Block size along N dimension\n):\n    # Program ID\n    pid_m = tl.program_id(axis=0)\n\n    # Calculate offsets\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_m = pid_m * stride_xm + tl.zeros([BLOCK_SIZE_N], dtype=tl.int64)\n\n    # Initialize accumulator\n    acc = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n\n    # Compute mean reduction\n    for offs in range(0, N, BLOCK_SIZE_N):\n        mask = offs_n + offs < N\n        offs_x = offs_m + (offs + offs_n) * stride_xn\n        x = tl.load(X_ptr + offs_x, mask=mask, other=0.0)\n        acc += x\n\n    # Reduce accumulator\n    acc = tl.sum(acc, axis=0)\n\n    # Store result\n    y_ptr = Y_ptr + pid_m * stride_y\n    tl.store(y_ptr, acc / N)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    Y = torch.empty(M, dtype=X.dtype, device=X.device)\n\n    # Define grid and block size\n    grid = lambda META: (M, )\n    BLOCK_SIZE_N = 256\n\n    # Launch kernel\n    mean_reduction_kernel[grid](\n        X, Y, M, N,\n        X.stride(0), X.stride(1), Y.stride(0),\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n    )\n    return Y",
    "operation": "mean_reduction",
    "input_shapes": [
      [
        1024,
        256
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    input_ptr = input_ptr + row * N\n    output_ptr = output_ptr + row\n\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    col = col_block * BLOCK_SIZE_N + col_offsets\n\n    mask = col < N\n    sum = tl.sum(tl.load(input_ptr + col, mask=mask), axis=0)\n\n    if col_block == 0:\n        tl.store(output_ptr, sum)\n\ndef sum_reduction(input, BLOCK_SIZE_N: int = 256):\n    M, N = input.shape\n    output = torch.empty(M, device=input.device, dtype=input.dtype)\n\n    grid = lambda meta: (M, triton.cdiv(N, meta['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](input, output, M, N, BLOCK_SIZE_N)\n    return output\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    input_ptr = input_ptr + row * N\n    output_ptr = output_ptr + row\n\n    col_offsets = tl.arange(0, BLOCK_SIZE_N)\n    col = col_block * BLOCK_SIZE_N + col_offsets\n\n    mask = col < N\n    sum = tl.sum(tl.load(input_ptr + col, mask=mask), axis=0)\n\n    if col_block == 0:\n        tl.store(output_ptr, sum)\n\ndef sum_reduction(input, BLOCK_SIZE_N: int = 256):\n    M, N = input.shape\n    output = torch.empty(M, device=input.device, dtype=input.dtype)\n\n    grid = lambda meta: (M, triton.cdiv(N, meta['BLOCK_SIZE_N']))\n    sum_reduction_kernel[grid](input, output, M, N, BLOCK_SIZE_N)\n    return output",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        1024,
        1024
      ]
    ],
    "output_shape": [
      1024
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the max reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n    \n    for n_offset in range(BLOCK_SIZE_N, N, BLOCK_SIZE_N):\n        offs_n = n_offset + tl.arange(0, BLOCK_SIZE_N)\n        val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    max_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, max_val)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n    \n    for n_offset in range(BLOCK_SIZE_N, N, BLOCK_SIZE_N):\n        offs_n = n_offset + tl.arange(0, BLOCK_SIZE_N)\n        val = tl.where(offs_n < N, tl.load(input_ptr + offs_n), -float('inf'))\n        max_val = tl.maximum(max_val, val)\n    \n    max_val = tl.max(max_val, axis=0)\n    tl.store(output_ptr, max_val)",
    "operation": "max_reduction",
    "input_shapes": [
      [
        128,
        1024
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the max reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col = tl.program_id(1)\n\n    row_offsets = row * N + tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.load(input_ptr + row_offsets, mask=row_offsets < M * N, other=-float('inf'))\n    for i in range(1, tl.cdiv(N, BLOCK_SIZE_N)):\n        row_offsets = row * N + tl.arange(0, BLOCK_SIZE_N) + i * BLOCK_SIZE_N\n        val = tl.load(input_ptr + row_offsets, mask=row_offsets < M * N, other=-float('inf'))\n        max_val = tl.maximum(max_val, val)\n\n    output_offsets = row\n    tl.store(output_ptr + output_offsets, max_val, mask=row < M)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef max_reduce_kernel(\n    input_ptr, \n    output_ptr, \n    M, \n    N, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col = tl.program_id(1)\n\n    row_offsets = row * N + tl.arange(0, BLOCK_SIZE_N)\n    max_val = tl.load(input_ptr + row_offsets, mask=row_offsets < M * N, other=-float('inf'))\n    for i in range(1, tl.cdiv(N, BLOCK_SIZE_N)):\n        row_offsets = row * N + tl.arange(0, BLOCK_SIZE_N) + i * BLOCK_SIZE_N\n        val = tl.load(input_ptr + row_offsets, mask=row_offsets < M * N, other=-float('inf'))\n        max_val = tl.maximum(max_val, val)\n\n    output_offsets = row\n    tl.store(output_ptr + output_offsets, max_val, mask=row < M)",
    "operation": "max_reduction",
    "input_shapes": [
      [
        128,
        256
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the sum reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    row_offs = row_idx\n    col_offs = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    input_ptr = input_ptr + row_offs * N\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offs < N\n        acc += tl.load(input_ptr + col_offs, mask=mask, other=0.0)\n        col_offs += BLOCK_SIZE_N\n\n    output_row_offs = row_idx\n    tl.store(output_ptr + output_row_offs, tl.sum(acc, axis=0))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sum_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n\n    row_offs = row_idx\n    col_offs = col_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    input_ptr = input_ptr + row_offs * N\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        mask = col_offs < N\n        acc += tl.load(input_ptr + col_offs, mask=mask, other=0.0)\n        col_offs += BLOCK_SIZE_N\n\n    output_row_offs = row_idx\n    tl.store(output_ptr + output_row_offs, tl.sum(acc, axis=0))",
    "operation": "sum_reduction",
    "input_shapes": [
      [
        128,
        128
      ]
    ],
    "output_shape": [
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the mean reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr, X_stride_M, X_stride_N,\n    Y_ptr,\n    M, N,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    X_ptr = X_ptr + row * X_stride_M + col_block * BLOCK_SIZE_N * X_stride_N\n\n    accumulator = tl.zeros([1], dtype=tl.float32)\n    offs = col_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = offs < N\n\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        x = tl.load(X_ptr + offs * X_stride_N, mask=mask, other=0.0)\n        accumulator += x\n        offs += BLOCK_SIZE_N\n        mask = offs < N\n\n    mean = accumulator / N.to(tl.float32)\n    tl.store(Y_ptr + row, mean)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    BLOCK_SIZE_N = 1024\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    Y = torch.empty(M, dtype=torch.float32, device=X.device)\n\n    mean_reduction_kernel[grid](\n        X, X.stride(0), X.stride(1),\n        Y,\n        M, N,\n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return Y\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_reduction_kernel(\n    X_ptr, X_stride_M, X_stride_N,\n    Y_ptr,\n    M, N,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    col_block = tl.program_id(1)\n\n    X_ptr = X_ptr + row * X_stride_M + col_block * BLOCK_SIZE_N * X_stride_N\n\n    accumulator = tl.zeros([1], dtype=tl.float32)\n    offs = col_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = offs < N\n\n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        x = tl.load(X_ptr + offs * X_stride_N, mask=mask, other=0.0)\n        accumulator += x\n        offs += BLOCK_SIZE_N\n        mask = offs < N\n\n    mean = accumulator / N.to(tl.float32)\n    tl.store(Y_ptr + row, mean)\n\ndef mean_reduction(X):\n    M, N = X.shape\n    BLOCK_SIZE_N = 1024\n    grid = (M, triton.cdiv(N, BLOCK_SIZE_N))\n    Y = torch.empty(M, dtype=torch.float32, device=X.device)\n\n    mean_reduction_kernel[grid](\n        X, X.stride(0), X.stride(1),\n        Y,\n        M, N,\n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n    return Y",
    "operation": "mean_reduction",
    "input_shapes": [
      [
        256,
        512
      ]
    ],
    "output_shape": [
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel that computes the min reduction along the last dimension of a 2D tensor with shape [M, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    mask = offs_n < N\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        offs_n += BLOCK_SIZE_N\n        mask = offs_n < N\n    \n    min_val = tl.min(min_val, axis=0)\n    tl.store(output_ptr, min_val)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr, output_ptr, \n    M: tl.constexpr, N: tl.constexpr, \n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    input_ptr = input_ptr + pid_m * N\n    output_ptr = output_ptr + pid_m\n    \n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    min_val = tl.full([BLOCK_SIZE_N], float('inf'), dtype=tl.float32)\n    mask = offs_n < N\n    \n    for _ in range(tl.cdiv(N, BLOCK_SIZE_N)):\n        val = tl.load(input_ptr + offs_n, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, val)\n        offs_n += BLOCK_SIZE_N\n        mask = offs_n < N\n    \n    min_val = tl.min(min_val, axis=0)\n    tl.store(output_ptr, min_val)",
    "operation": "min_reduction",
    "input_shapes": [
      [
        512,
        128
      ]
    ],
    "output_shape": [
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        256,
        256
      ],
      [
        256,
        512
      ]
    ],
    "output_shape": [
      256,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Program ID and grid coordinates\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and accumulator\n    A_ptrs = A_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    B_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B with masking for edge cases\n        A = tl.load(A_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        B = tl.load(B_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n\n        # Update accumulator\n        acc += tl.dot(A, B)\n\n        # Advance pointers for A and B\n        A_ptrs += BLOCK_SIZE_K * stride_ak\n        B_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate output pointers and store result with masking\n    C_ptrs = C_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n    mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(C_ptrs, acc, mask=mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Program ID and grid coordinates\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and accumulator\n    A_ptrs = A_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    B_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B with masking for edge cases\n        A = tl.load(A_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        B = tl.load(B_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n\n        # Update accumulator\n        acc += tl.dot(A, B)\n\n        # Advance pointers for A and B\n        A_ptrs += BLOCK_SIZE_K * stride_ak\n        B_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate output pointers and store result with masking\n    C_ptrs = C_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n    mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(C_ptrs, acc, mask=mask)",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        256
      ],
      [
        256,
        512
      ]
    ],
    "output_shape": [
      512,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract program IDs\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C blocks\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        c += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract program IDs\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C blocks\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        c += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        256
      ],
      [
        256,
        256
      ]
    ],
    "output_shape": [
      512,
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program's block indices\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the block's starting position in the matrices\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers to the block data\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize the accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the data for this block\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product for this block\n        accumulator += tl.dot(a, b)\n\n        # Advance the pointers for the next block\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef matmul(a, b):\n    # Check the input shapes\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate the output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Calculate the grid size\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program's block indices\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the block's starting position in the matrices\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers to the block data\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize the accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the data for this block\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product for this block\n        accumulator += tl.dot(a, b)\n\n        # Advance the pointers for the next block\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef matmul(a, b):\n    # Check the input shapes\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate the output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Calculate the grid size\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        256
      ],
      [
        256,
        512
      ]
    ],
    "output_shape": [
      512,
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Create pointers to the first elements of A and B blocks\n    A_block_ptr = A_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    B_block_ptr = B_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n[None, :] * stride_bn)\n\n    # Initialize accumulator for the C block\n    C_block = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks with masking for edge cases\n        A = tl.load(A_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n        B = tl.load(B_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n\n        # Update accumulator\n        C_block += tl.dot(A, B)\n\n        # Advance pointers for A and B blocks\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Create pointer to the C block\n    C_block_ptr = C_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn)\n\n    # Store the C block with masking for edge cases\n    tl.store(C_block_ptr, C_block, mask=(block_m[:, None] < M) & (block_n[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Create pointers to the first elements of A and B blocks\n    A_block_ptr = A_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    B_block_ptr = B_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n[None, :] * stride_bn)\n\n    # Initialize accumulator for the C block\n    C_block = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks with masking for edge cases\n        A = tl.load(A_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n        B = tl.load(B_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n\n        # Update accumulator\n        C_block += tl.dot(A, B)\n\n        # Advance pointers for A and B blocks\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Create pointer to the C block\n    C_block_ptr = C_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn)\n\n    # Store the C block with masking for edge cases\n    tl.store(C_block_ptr, C_block, mask=(block_m[:, None] < M) & (block_n[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        256
      ],
      [
        256,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program's block indices\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the block's row and column offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the blocks of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the block's dot product\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid for kernel launch\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program's block indices\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the block's row and column offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the blocks of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the block's dot product\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid for kernel launch\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        256
      ],
      [
        256,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate output pointers\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store results with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate output pointers\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store results with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        128
      ],
      [
        128,
        128
      ]
    ],
    "output_shape": [
      512,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Calculate memory offsets\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak)\n        b_ptrs = b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n    # Calculate output memory offset\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store result with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Calculate memory offsets\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak)\n        b_ptrs = b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n    # Calculate output memory offset\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store result with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        256,
        128
      ],
      [
        128,
        128
      ]
    ],
    "output_shape": [
      256,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Program ID and grid size\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block offsets\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    B_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    C = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B with masking for edge cases\n        A = tl.load(A_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        B = tl.load(B_ptrs, mask=(offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)\n\n        # Perform computation\n        C += tl.dot(A, B)\n\n        # Update pointers for next iteration\n        A_ptrs += BLOCK_SIZE_K * stride_ak\n        B_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store result to C with masking for edge cases\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    C_ptrs = C_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(C_ptrs, C, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Program ID and grid size\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block offsets\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    B_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    C = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B with masking for edge cases\n        A = tl.load(A_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        B = tl.load(B_ptrs, mask=(offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)\n\n        # Perform computation\n        C += tl.dot(A, B)\n\n        # Update pointers for next iteration\n        A_ptrs += BLOCK_SIZE_K * stride_ak\n        B_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store result to C with masking for edge cases\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    C_ptrs = C_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(C_ptrs, C, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        512
      ],
      [
        512,
        512
      ]
    ],
    "output_shape": [
      128,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program ID\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the current block of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block of C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the current block of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance the pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate the offset for the current block of C\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n\n    # Store the result to C\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the shapes are compatible\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n\n    # Get the shapes of the matrices\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate the output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Calculate the grid size\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract the current program ID\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate the offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the current block of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block of C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the current block of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform the dot product for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance the pointers for A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Calculate the offset for the current block of C\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n\n    # Store the result to C\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the shapes are compatible\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n\n    # Get the shapes of the matrices\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate the output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Calculate the grid size\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        256,
        128
      ],
      [
        128,
        512
      ]
    ],
    "output_shape": [
      256,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Calculate memory offsets\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak)\n        b_ptrs = b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n    # Calculate memory offsets for output\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store results with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Calculate memory offsets\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak)\n        b_ptrs = b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n\n        # Load data with masking\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        accumulator += tl.dot(a, b)\n\n    # Calculate memory offsets for output\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n\n    # Store results with masking\n    tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        512
      ],
      [
        512,
        256
      ]
    ],
    "output_shape": [
      128,
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Create pointers to the first elements of the blocks\n    a_block_ptr = a_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    b_block_ptr = b_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n * stride_bn)\n\n    # Initialize accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks from A and B with masking\n        a = tl.load(a_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n        b = tl.load(b_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n\n        # Update accumulator\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers\n        a_block_ptr += BLOCK_SIZE_K * stride_ak\n        b_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Store the result to C\n    c_block_ptr = c_ptr + (block_m[:, None] * stride_cm + block_n * stride_cn)\n    tl.store(c_block_ptr, accumulator, mask=(block_m[:, None] < M) & (block_n < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Create pointers to the first elements of the blocks\n    a_block_ptr = a_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    b_block_ptr = b_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n * stride_bn)\n\n    # Initialize accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks from A and B with masking\n        a = tl.load(a_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n        b = tl.load(b_block_ptr, mask=(k + tl.arange(0, BLOCK_SIZE_K))[:, None] < K, other=0.0)\n\n        # Update accumulator\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers\n        a_block_ptr += BLOCK_SIZE_K * stride_ak\n        b_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Store the result to C\n    c_block_ptr = c_ptr + (block_m[:, None] * stride_cm + block_n * stride_cn)\n    tl.store(c_block_ptr, accumulator, mask=(block_m[:, None] < M) & (block_n < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        128
      ],
      [
        128,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_PTR, B_PTR, C_PTR,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_PTR + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B_PTR + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(C_PTR.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_PTR + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_PTR, B_PTR, C_PTR,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_PTR + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B_PTR + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(C_PTR.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_PTR + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        512
      ],
      [
        512,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid size\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the current block of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block of C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load current block of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers to the next block of A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result in C\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the shapes are compatible for matrix multiplication\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Calculate grid size\n    grid_m = triton.cdiv(M, BLOCK_SIZE_M)\n    grid_n = triton.cdiv(N, BLOCK_SIZE_N)\n\n    # Launch kernel\n    matmul_kernel[grid_m, grid_n](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid size\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate offsets for the current block\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the current block of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the current block of C\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load current block of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation for the current block\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers to the next block of A and B\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result in C\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check if the shapes are compatible for matrix multiplication\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Calculate grid size\n    grid_m = triton.cdiv(M, BLOCK_SIZE_M)\n    grid_n = triton.cdiv(N, BLOCK_SIZE_N)\n\n    # Launch kernel\n    matmul_kernel[grid_m, grid_n](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        256,
        512
      ],
      [
        512,
        512
      ]
    ],
    "output_shape": [
      256,
      512
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate the block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the blocks of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the block\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Update accumulator\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers to the next block\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid dimensions\n    grid_m = triton.cdiv(M, BLOCK_SIZE_M)\n    grid_n = triton.cdiv(N, BLOCK_SIZE_N)\n\n    # Launch kernel\n    matmul_kernel[grid_m, grid_n](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate the block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to the blocks of A and B\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Initialize accumulator for the block\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Update accumulator\n        accumulator += tl.dot(a, b)\n\n        # Advance pointers to the next block\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    # Define grid dimensions\n    grid_m = triton.cdiv(M, BLOCK_SIZE_M)\n    grid_n = triton.cdiv(N, BLOCK_SIZE_N)\n\n    # Launch kernel\n    matmul_kernel[grid_m, grid_n](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        256
      ],
      [
        256,
        512
      ]
    ],
    "output_shape": [
      512,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        512
      ],
      [
        512,
        256
      ]
    ],
    "output_shape": [
      128,
      256
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C blocks\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] < K - k) & (offs_am[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        c += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define grid dimensions\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # Program ID and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize pointers to A, B, and C blocks\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] < K - k) & (offs_am[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n\n        # Perform computation\n        c += tl.dot(a, b)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    # Check dimensions\n    assert a.shape[1] == b.shape[0]\n    M, K = a.shape\n    K, N = b.shape\n\n    # Allocate output\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define grid dimensions\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    return c",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        256
      ],
      [
        256,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program IDs and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        block_k = k + tl.arange(0, BLOCK_SIZE_K)\n\n        # Load A and B blocks with masking\n        a = tl.load(a_ptr + (block_m[:, None] * stride_am + block_k[None, :] * stride_ak), mask=(block_m[:, None] < M) & (block_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptr + (block_k[:, None] * stride_bk + block_n[None, :] * stride_bn), mask=(block_k[:, None] < K) & (block_n[None, :] < N), other=0.0)\n\n        # Perform dot product\n        acc += tl.dot(a, b)\n\n    # Store result with masking\n    tl.store(c_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn), acc, mask=(block_m[:, None] < M) & (block_n[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program IDs and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        block_k = k + tl.arange(0, BLOCK_SIZE_K)\n\n        # Load A and B blocks with masking\n        a = tl.load(a_ptr + (block_m[:, None] * stride_am + block_k[None, :] * stride_ak), mask=(block_m[:, None] < M) & (block_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptr + (block_k[:, None] * stride_bk + block_n[None, :] * stride_bn), mask=(block_k[:, None] < K) & (block_n[None, :] < N), other=0.0)\n\n        # Perform dot product\n        acc += tl.dot(a, b)\n\n    # Store result with masking\n    tl.store(c_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn), acc, mask=(block_m[:, None] < M) & (block_n[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        128
      ],
      [
        128,
        128
      ]
    ],
    "output_shape": [
      512,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N]\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Define program IDs and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Load data and compute\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak),\n                    mask=(offs_am[:, None] < M) & ((k + offs_k[None, :]) < K), other=0.0)\n        b = tl.load(b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn),\n                    mask=((k + offs_k[:, None]) < K) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b)\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    tl.store(c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn),\n             accumulator,\n             mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define BLOCK_SIZE constants\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 256\nBLOCK_SIZE_K = 64\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr = BLOCK_SIZE_M,\n    BLOCK_SIZE_N: tl.constexpr = BLOCK_SIZE_N,\n    BLOCK_SIZE_K: tl.constexpr = BLOCK_SIZE_K,\n):\n    # Define program IDs and grid dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Calculate block offsets\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulators\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Load data and compute\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptr + (offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak),\n                    mask=(offs_am[:, None] < M) & ((k + offs_k[None, :]) < K), other=0.0)\n        b = tl.load(b_ptr + ((k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn),\n                    mask=((k + offs_k[:, None]) < K) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b)\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    tl.store(c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn),\n             accumulator,\n             mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        128,
        128
      ],
      [
        128,
        128
      ]
    ],
    "output_shape": [
      128,
      128
    ],
    "optimization_level": "none",
    "compiles": true,
    "error_message": null
  },
  {
    "prompt": "You are an expert at writing high-performance Triton kernels.\n\nTask: Write a Triton kernel for matrix multiplication of tensors with shapes [M, K] and [K, N] using tile-based computation with shared memory\n\nRequirements:\n- Write clean, efficient Triton code\n- Include proper imports (triton, triton.language as tl)\n- Use appropriate BLOCK_SIZE constants\n- Handle edge cases with proper masking\n- Include the @triton.jit decorator\n\nProvide only the kernel code without explanation.",
    "response": "```python\nimport triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Initialize pointers to A and B blocks\n    A_block_ptr = A_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    B_block_ptr = B_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n[None, :] * stride_bn)\n\n    # Perform tile-based computation\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B tiles with masking\n        A = tl.load(A_block_ptr, mask=(block_m[:, None] < M) & (tl.arange(0, BLOCK_SIZE_K)[None, :] < K - k), other=0.0)\n        B = tl.load(B_block_ptr, mask=(tl.arange(0, BLOCK_SIZE_K)[:, None] < K - k) & (block_n[None, :] < N), other=0.0)\n\n        # Update accumulators\n        acc += tl.dot(A, B)\n\n        # Move pointers to next A and B tiles\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Store result to C with masking\n    C_block_ptr = C_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn)\n    tl.store(C_block_ptr, acc, mask=(block_m[:, None] < M) & (block_n[None, :] < N))\n```",
    "kernel_code": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    **meta\n):\n    # Define program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block IDs\n    block_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    block_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Initialize pointers to A and B blocks\n    A_block_ptr = A_ptr + (block_m[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K) * stride_ak)\n    B_block_ptr = B_ptr + (tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + block_n[None, :] * stride_bn)\n\n    # Perform tile-based computation\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B tiles with masking\n        A = tl.load(A_block_ptr, mask=(block_m[:, None] < M) & (tl.arange(0, BLOCK_SIZE_K)[None, :] < K - k), other=0.0)\n        B = tl.load(B_block_ptr, mask=(tl.arange(0, BLOCK_SIZE_K)[:, None] < K - k) & (block_n[None, :] < N), other=0.0)\n\n        # Update accumulators\n        acc += tl.dot(A, B)\n\n        # Move pointers to next A and B tiles\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Store result to C with masking\n    C_block_ptr = C_ptr + (block_m[:, None] * stride_cm + block_n[None, :] * stride_cn)\n    tl.store(C_block_ptr, acc, mask=(block_m[:, None] < M) & (block_n[None, :] < N))",
    "operation": "matmul",
    "input_shapes": [
      [
        512,
        512
      ],
      [
        512,
        512
      ]
    ],
    "output_shape": [
      512,
      512
    ],
    "optimization_level": "basic",
    "compiles": true,
    "error_message": null
  }
]