.version 7.5
.target sm_86
.address_size 64

.visible .entry gemm4x4_1d_tiled(
    .param .u64 A,
    .param .u64 B,
    .param .u64 C
)
{
    .reg .f32 a_tile<4>;    // Renamed to avoid conflict
    .reg .f32 b_tile<4>;    // Renamed to avoid conflict
    .reg .f32 c<16>;        // Result matrix
    .reg .u64 a_ptr, b_ptr, c_ptr;
    .reg .u32 k;
    .reg .u64 a_offset, b_offset;
    .reg .pred p;
    
    // Load pointers
    ld.param.u64 a_ptr, [A];
    ld.param.u64 b_ptr, [B];
    ld.param.u64 c_ptr, [C];
    
    // Initialize C to zeros
    mov.f32 c0, 0.0;
    mov.f32 c1, 0.0;
    mov.f32 c2, 0.0;
    mov.f32 c3, 0.0;
    mov.f32 c4, 0.0;
    mov.f32 c5, 0.0;
    mov.f32 c6, 0.0;
    mov.f32 c7, 0.0;
    mov.f32 c8, 0.0;
    mov.f32 c9, 0.0;
    mov.f32 c10, 0.0;
    mov.f32 c11, 0.0;
    mov.f32 c12, 0.0;
    mov.f32 c13, 0.0;
    mov.f32 c14, 0.0;
    mov.f32 c15, 0.0;
    
    // Tiling loop over K dimension (k=0,1,2,3)
    mov.u32 k, 0;
    
k_loop:
    // Calculate offsets for current k
    cvt.u64.u32 a_offset, k;
    mul.lo.u64 a_offset, a_offset, 4;     // k * sizeof(float)
    
    cvt.u64.u32 b_offset, k;
    mul.lo.u64 b_offset, b_offset, 16;    // k * 4 * sizeof(float)
    
    // Load tiles from A (one element from each row)
    ld.global.f32 a_tile0, [a_ptr + a_offset];         // A[0][k]
    ld.global.f32 a_tile1, [a_ptr + 16 + a_offset];    // A[1][k]
    ld.global.f32 a_tile2, [a_ptr + 32 + a_offset];    // A[2][k]
    ld.global.f32 a_tile3, [a_ptr + 48 + a_offset];    // A[3][k]
    
    // Load tiles from B (a full row)
    ld.global.f32 b_tile0, [b_ptr + b_offset];         // B[k][0]
    ld.global.f32 b_tile1, [b_ptr + b_offset + 4];     // B[k][1]
    ld.global.f32 b_tile2, [b_ptr + b_offset + 8];     // B[k][2]
    ld.global.f32 b_tile3, [b_ptr + b_offset + 12];    // B[k][3]
    
    // Compute partial products for current tile
    // C[0,0:3] += A[0][k] * B[k][0:3]
    fma.rn.f32 c0, a_tile0, b_tile0, c0;
    fma.rn.f32 c1, a_tile0, b_tile1, c1;
    fma.rn.f32 c2, a_tile0, b_tile2, c2;
    fma.rn.f32 c3, a_tile0, b_tile3, c3;
    
    // C[1,0:3] += A[1][k] * B[k][0:3]
    fma.rn.f32 c4, a_tile1, b_tile0, c4;
    fma.rn.f32 c5, a_tile1, b_tile1, c5;
    fma.rn.f32 c6, a_tile1, b_tile2, c6;
    fma.rn.f32 c7, a_tile1, b_tile3, c7;
    
    // C[2,0:3] += A[2][k] * B[k][0:3]
    fma.rn.f32 c8, a_tile2, b_tile0, c8;
    fma.rn.f32 c9, a_tile2, b_tile1, c9;
    fma.rn.f32 c10, a_tile2, b_tile2, c10;
    fma.rn.f32 c11, a_tile2, b_tile3, c11;
    
    // C[3,0:3] += A[3][k] * B[k][0:3]
    fma.rn.f32 c12, a_tile3, b_tile0, c12;
    fma.rn.f32 c13, a_tile3, b_tile1, c13;
    fma.rn.f32 c14, a_tile3, b_tile2, c14;
    fma.rn.f32 c15, a_tile3, b_tile3, c15;
    
    // Increment k and check if we've processed all tiles
    add.u32 k, k, 1;
    setp.lt.u32 p, k, 4;
    @p bra k_loop;
    
    // Store results to matrix C
    st.global.f32 [c_ptr], c0;
    st.global.f32 [c_ptr+4], c1;
    st.global.f32 [c_ptr+8], c2;
    st.global.f32 [c_ptr+12], c3;
    st.global.f32 [c_ptr+16], c4;
    st.global.f32 [c_ptr+20], c5;
    st.global.f32 [c_ptr+24], c6;
    st.global.f32 [c_ptr+28], c7;
    st.global.f32 [c_ptr+32], c8;
    st.global.f32 [c_ptr+36], c9;
    st.global.f32 [c_ptr+40], c10;
    st.global.f32 [c_ptr+44], c11;
    st.global.f32 [c_ptr+48], c12;
    st.global.f32 [c_ptr+52], c13;
    st.global.f32 [c_ptr+56], c14;
    st.global.f32 [c_ptr+60], c15;
    
    ret;
}
